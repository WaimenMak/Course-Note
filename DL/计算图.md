# 计算图

在深度学习框架里底层实现`Back-propagation`算法是通过构建计算图完成，普通的一元运算和二元运算可以简单地通过二叉树实现，

```python
class Node():
    def __init__(self):
        self.last_right = None
        self.last_left = None
        self.next = None
        self.value = None
        self.grad = None
        self.sub_grad = None
        self.require_grad = True
```

二叉树的一个节点的属性如上，我们可以通过继承上面这个基类去设计更多类型的节点，如变量节点`(Variable)`，常数节点`(Constant)`，操作符节点`(Operator)`，输入节点`Input`等等。

每个节点设计有如下方法：

```python
def output_val(self):
    pass

def compute_gradient(self):
    pass

def update_param(self):
    pass
```



### Dimension Analysis

在这里，我设计的求导方式是基于标量对向量或矩阵进行反向传播，因此`Backward`函数要作用在最后的输出节点才有意义。

### Dense层的设计

这里本来打算把全连接层直接设计为一个节点，输入节点加上一列全为1，权重矩阵加上一行b，这样可以减少整个二叉树的大小，遍历起来速度应该也会快些（空间换时间）。`weight_bias`储存weight和bias的值，`self.sub_grad`储存的是当前输入的gradient所以是weight的转置，省略掉了bias的gradient。 后面操作发现`Dense`节点的属性如果只有sub_grad和grad无法通过简单的反向传播计算当前节点的梯度，如果这样设计不够general，例如本来当前节点compute gradient可以把下个节点的sub gradient算出来，但Dense层算出来下一层sub gradient和这一层的gradient通过chain_rule相乘得到的不是下一层的gradient，因为Dense层算出来的sub gradient只是 $\frac{\partial L}{\partial A}$ 需要多一个属性储存 $\frac{\partial A}{\partial w}$ ,还有一个属性储存 $\frac{\partial L}{\partial w}$ ，这样结构上就和普通运算的节点不一样，所以没那么general。所以后面采取的方法还是直接设计一个Dense函数，直接用原来的节点搭起来一个全连接层。
